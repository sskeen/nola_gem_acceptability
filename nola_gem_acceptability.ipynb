{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzgKP4GEalxQ"
   },
   "source": [
    "## Acceptability, feasibility, and user experiences of NOLA Gem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRMega9UavJZ"
   },
   "source": [
    "_WIP - NOT FOR DISTRIBUTION_\n",
    "\n",
    "_Vizualization scripts for pilot trial ($N$ = 32) analyses of acceptability, feasibility, and user experience with [NOLA Gem](https://www.researchprotocols.org/2023/1/e47151/authors): a geospatially customizable culturally tailored JITAI for violence-affected people living with HIV._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5O-o5plbL4_"
   },
   "source": [
    "> `nola_gem_acceptability.ipynb`<br>\n",
    "> Simone J. Skeen (07-12-2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrrkzGPlbaaQ"
   },
   "source": [
    "### 1. Prepare\n",
    "Installs, imports, requisite packages; customizes outputs.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gu7SLsTr09uq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install ollama\n",
    "#!pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atjpV2bUbhzM"
   },
   "outputs": [],
   "source": [
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "#import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "#from irrCAC.raw import CAC\n",
    "#from google.colab import drive\n",
    "#from langchain_community.llms import Ollama\n",
    "from matplotlib import cm\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "pd.set_option(\n",
    "    'display.max_columns',\n",
    "    None,\n",
    "    )\n",
    "\n",
    "pd.set_option(\n",
    "    'display.max_rows',\n",
    "    None,\n",
    "    )\n",
    "\n",
    "#warnings.simplefilter(\n",
    "#    action = 'ignore',\n",
    "#    category = FutureWarning,\n",
    "#    )\n",
    "\n",
    "for c in (FutureWarning, UserWarning):\n",
    "    warnings.simplefilter(\n",
    "        action = 'ignore',\n",
    "        category = c,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJHePSXTcFyM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#fm.fontManager.ttflist += fm.createFontList(['Arial.ttf'])\n",
    "\n",
    "fm.fontManager.addfont('/content/drive/MyDrive/Colab/Arial.ttf')\n",
    "plt.rcParams['font.family'] = 'Arial'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnpjLWe01Pb-"
   },
   "source": [
    "**Ollama**<br>\n",
    "http://localhost:11434/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mount gdrive (cloud)\n",
    "\n",
    "drive.mount(\n",
    "    '/content/drive',\n",
    "    force_remount = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jupyter Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set wd (local)\n",
    "\n",
    "wd = 'C:/Users/sskee/OneDrive/Documents/02_tulane/01_research/nola_gem/dissem/skeen,etal_acceptability/code'\n",
    "os.chdir(wd)\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oAjZy230Hbg"
   },
   "source": [
    "### 2. Write\n",
    "Defines `qualitative.py` module.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile qualitative.py\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def code_texts_deductively_ollama(df, alias, text_column, endpoint_url, prompt_template, model_name):\n",
    "    '''\n",
    "    Classifies each row of 'text' column in provided df in accord with human-specified prompt,\n",
    "    includes chain-of-thought reasoning, returning explanations for classification decision.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        df containing the text to classify.\n",
    "    alias : str\n",
    "        alias (for brevity) of the qualitative code to be applied.\n",
    "    text_column : str\n",
    "        column name in df containing the text to be analyzed.\n",
    "    endpoint_url : str\n",
    "        URL where locally hosted LLM runs.\n",
    "    prompt_template : str\n",
    "        prompt text with a placeholder (e.g. '{text}') where the row's text will be inserted.\n",
    "    model_name : str\n",
    "        model tasked with qualitative deductive coding.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "   df : pd.DataFrame\n",
    "        The original df with two new columns per deductive code: '{alias}_llm' (either \"0\" or \"1\")\n",
    "        and '{alias}_expl' (the cahin-of-thought explanation)\n",
    "    '''\n",
    "\n",
    "    label_column = f'{alias}_llm'\n",
    "    explanation_column = f'{alias}_expl'\n",
    "\n",
    "    # insert cols - necessary for .update() downstream\n",
    "    \n",
    "    df[label_column] = None\n",
    "    df[explanation_column] = None    \n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        row_text = row[text_column]\n",
    "\n",
    "        unique_id = f\"[Row ID: {idx}]\"\n",
    "        prompt = f\"{unique_id}\\n\\n\" + prompt_template.format(text = row_text)\n",
    "\n",
    "        response = requests.post(\n",
    "            endpoint_url,\n",
    "            headers = {'Content-Type': 'application/json'},\n",
    "            json = {\n",
    "                'model': model_name,\n",
    "                'prompt': prompt,\n",
    "                'stream': False\n",
    "            })\n",
    "\n",
    "        print(f\"\\n--- Index {idx} ---\")\n",
    "        print(\"Prompt:\")\n",
    "        print(prompt)\n",
    "        print(f\"Status: {response.status_code}\")\n",
    "        print(\"Raw response:\")\n",
    "        print(response.text)\n",
    "\n",
    "        label = None\n",
    "        explanation = None\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            try:\n",
    "                result_json = response.json()\n",
    "                raw_response_str = result_json.get('response', ' ')\n",
    "\n",
    "                cleaned_str = raw_response_str.strip().replace(\"```json\", \" \").replace(\"```\", \" \").strip()\n",
    "                parsed_output = json.loads(cleaned_str)\n",
    "\n",
    "                label = parsed_output.get(label_column)\n",
    "                explanation = parsed_output.get(explanation_column)\n",
    "\n",
    "            except (json.JSONDecodeError, KeyError, TypeError) as e:\n",
    "                print(\"JSON error:\", e)\n",
    "                print(\"Bad string:\")\n",
    "                print(cleaned_str)\n",
    "\n",
    "        # save results by row index for correct matching\n",
    "        \n",
    "        results.append({\n",
    "            'idx': idx,\n",
    "            label_column: label,\n",
    "            explanation_column: explanation\n",
    "            })\n",
    "\n",
    "        # impose delay - avoid model caching errors\n",
    "        \n",
    "        time.sleep(0.25)        \n",
    "        \n",
    "    # create result df - align to input df by row index\n",
    "    \n",
    "    result_df = pd.DataFrame(results).set_index('idx')\n",
    "    df.update(result_df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd\n",
    "from qualitative import(\n",
    "    code_texts_deductively_ollama,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Describe\n",
    "Aggregates, tabulates, paradata.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Daily diary skills recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = 'C:/Users/sskee/OneDrive/Documents/02_tulane/01_research/nola_gem/dissem/skeen,etal_acceptability/data/paradata'\n",
    "os.chdir(wd)\n",
    "%pwd\n",
    "\n",
    "d_diary = pd.read_csv(\n",
    "    'diary_response.csv',\n",
    "#    index_col = [0],\n",
    "    )\n",
    "\n",
    "valid_tx_ids = {\n",
    "    'g1008', 'g1011', 'g1014', 'g1019', 'g1023', 'g1025', 'g1027',\n",
    "    'g1030', 'g1033', 'g1034', 'g1035', 'g1037', 'g1038', 'g1039',\n",
    "    'g1040', 'g1042', 'g1044', 'g1046', 'g1047', 'g1049', 'g1050',\n",
    "    'g1051', 'gg1045'\n",
    "    }\n",
    "\n",
    "# reset idx\n",
    "\n",
    "d = d_diary[d_diary['username'].isin(valid_tx_ids)]\n",
    "d.reset_index(inplace = True, drop = True)\n",
    "\n",
    "\n",
    "# convert datetime\n",
    "\n",
    "d['created_at'] = pd.to_datetime(\n",
    "    d['created_at'], utc = True\n",
    "    ).dt.tz_convert(None).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# inspect\n",
    "\n",
    "d.shape\n",
    "d.info()\n",
    "d.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(d.columns.tolist())\n",
    "#d.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommendation_skill_selection - inspect\n",
    "\n",
    "d['recommendation_skill_selection'] = d['recommendation_skill_selection'].fillna('none')\n",
    "print(d['recommendation_skill_selection'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get n \n",
    "\n",
    "n = d['recommendation_skill_selection'].value_counts(dropna = False)\n",
    "#pct = d['recommendation_skill_selection'].value_counts(\n",
    "#    normalize = True, \n",
    "#    dropna = False,\n",
    "#    ) * 100\n",
    "\n",
    "# merge, display\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    'n': n,\n",
    "#    'pct': pct.round(2) ### round to 2 decimal places\n",
    "    })\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get %: n / N skills recommendations \n",
    "\n",
    "10 / 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Completed skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv(\n",
    "    'UserSummary.csv',\n",
    "#    index_col = [0],\n",
    "    )\n",
    "\n",
    "# inspect\n",
    "\n",
    "d.shape\n",
    "d.info()\n",
    "d.head(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * spent_minutes - mdn / iqr\n",
    "\n",
    "skills_min = [\n",
    "    'Breathing Retraining spent_minutes', \n",
    "    'Journal Writing spent_minutes', \n",
    "    'Adaptive Coping spent_minutes',\n",
    "    'Common Irrational Thoughts spent_minutes', \n",
    "    'Choosing a Coping Strategy spent_minutes', \n",
    "    'Set a SMART Goal spent_minutes',\n",
    "    'Meditation spent_minutes',\n",
    "    ]\n",
    "\n",
    "# compute mdn / iqr\n",
    "\n",
    "summary = {}\n",
    "\n",
    "for skill in skills_min:\n",
    "    non_zero = d[d[skill] > 0][skill] ### parse for >0 values\n",
    "    zero_count = (d[skill] == 0).sum() ### count 0 values\n",
    "    mdn = non_zero.median()\n",
    "    q1 = non_zero.quantile(0.25)\n",
    "    q3 = non_zero.quantile(0.75)\n",
    "    val_range = non_zero.max() - non_zero.min() ### range of >0 values\n",
    "    \n",
    "    summary[skill] = {\n",
    "        'mdn': mdn,\n",
    "#        'q1': q1,\n",
    "#        'q3': q3,\n",
    "        'zero_count': zero_count,\n",
    "        'non_zero_range': val_range,       \n",
    "        }\n",
    "\n",
    "# tabulate, display\n",
    "\n",
    "summary = pd.DataFrame(summary).T ### transpose for interpretability\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * completion_count - mdn / iqr\n",
    "\n",
    "skills_complete = [\n",
    "    'Breathing Retraining completion_count', \n",
    "    'Journal Writing completion_count',\n",
    "    'Adaptive Coping completion_count',\n",
    "    'Common Irrational Thoughts completion_count',\n",
    "    'Choosing a Coping Strategy completion_count',\n",
    "    'Set a SMART Goal completion_count',\n",
    "    'Meditation completion_count',\n",
    "    ]\n",
    "\n",
    "# compute m (sd)\n",
    "\n",
    "summary = {}\n",
    "\n",
    "for skill in skills_complete:\n",
    "    non_zero = d[d[skill] > 0][skill] ### parse for >0 values\n",
    "    zero_count = (d[skill] == 0).sum() ### count 0 values\n",
    "#    mean_val = d[col].mean()\n",
    "#    std_val = d[col].std()\n",
    "    mean_val = non_zero.mean()\n",
    "    std_val = non_zero.std()\n",
    "    summary[skill] = {\n",
    "        'mean': mean_val,\n",
    "        'std': std_val,\n",
    "        'zero_count': zero_count,\n",
    "        }\n",
    "\n",
    "# tabulate, display\n",
    "\n",
    "summary = pd.DataFrame(summary).T\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get n\n",
    "\n",
    "sums = {}\n",
    "\n",
    "for skill in skills_complete:\n",
    "    total = d[skill].sum()\n",
    "    column_sums[skill] = total\n",
    "\n",
    "# tabulate, display\n",
    "\n",
    "d_sums = pd.DataFrame.from_dict(\n",
    "    sums, \n",
    "    orient = 'index', \n",
    "    columns = ['sum'],\n",
    "    )\n",
    "print(d_sums)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get %: n / N skills completed\n",
    "\n",
    "19 / 205"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HMbHCX0cRbR",
    "tags": []
   },
   "source": [
    "### 3. Visualize\n",
    "Plots acceptability, usability, etc. findings.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5WLb04Ycg0-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/My Drive/Colab/nola_gem_acceptability/inputs/data\n",
    "\n",
    "d = pd.read_csv(\n",
    "    'nola_gem_acceptability_no_loc_tx.csv',\n",
    "    index_col = [0],\n",
    "    )\n",
    "\n",
    "d.info()\n",
    "d.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pz8ZAe2_1B_W"
   },
   "outputs": [],
   "source": [
    "d[[\n",
    "    'intervention1_rev',\n",
    "    'intervention2_rev',\n",
    "    'intervention3_rev',\n",
    "    'intervention4_rev',\n",
    "    'intervention5_rev',\n",
    "    ]].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njFUb27COdCd"
   },
   "outputs": [],
   "source": [
    "%cd ../../outputs/figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RS6CgqU2RO4r",
    "tags": []
   },
   "source": [
    "##### **Fig. 1.** NOLA Gem acceptability: domain general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H40cGAdiz-36"
   },
   "outputs": [],
   "source": [
    "# aesthetics\n",
    "\n",
    "sns.set_style(\n",
    "    style = 'whitegrid',\n",
    "    rc = None,\n",
    "    )\n",
    "\n",
    "int_cols = [\n",
    "    'intervention1_rev',\n",
    "    'intervention2_rev',\n",
    "    'intervention3_rev',\n",
    "    'intervention4_rev',\n",
    "    'intervention5_rev',\n",
    "    ]\n",
    "\n",
    "# customize x-axis labels\n",
    "\n",
    "x_positions = range(len(int_cols))\n",
    "x_labels = [\n",
    "    'useful',\n",
    "    'easy to understand',\n",
    "    'satisfactory',\n",
    "    'likeable',\n",
    "    'visually appealing',\n",
    "    ]\n",
    "\n",
    "# customize y-axis labels\n",
    "\n",
    "y_labels = {\n",
    "    1: 'disagree',\n",
    "    2: 'somewhat\\ndisagree',\n",
    "    3: 'somewhat\\nagree',\n",
    "    4: 'agree',\n",
    "    }\n",
    "\n",
    "# uniform color: ng_blue\n",
    "\n",
    "ng_blue = '#7eb0d5'\n",
    "\n",
    "# plot\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8, 5))\n",
    "\n",
    "# loop over int1-int5\n",
    "\n",
    "for i, col in enumerate(int_cols):\n",
    "    y_vals = d[col].astype(float) ### coerce numeric\n",
    "    x_jittered = np.random.normal(\n",
    "        loc = i,\n",
    "        scale = 0.16, ### inject jitter\n",
    "        size = len(y_vals),\n",
    "        )\n",
    "\n",
    "    # color = custom_colors[col]\n",
    "\n",
    "    ax.scatter(\n",
    "        x_jittered,\n",
    "        y_vals,\n",
    "        alpha = 0.6,\n",
    "        s = 35,\n",
    "        color = ng_blue,\n",
    "        #edgecolors = 'k',\n",
    "        linewidths = 0.5,\n",
    "        label = col,\n",
    "        )\n",
    "\n",
    "    # overlay m (sd) by int*\n",
    "\n",
    "    mean_val = y_vals.mean()\n",
    "    std_val = y_vals.std()\n",
    "\n",
    "    ax.errorbar(\n",
    "        i,\n",
    "        mean_val,\n",
    "        yerr = std_val,\n",
    "        fmt = 'D',\n",
    "        color = ng_blue,\n",
    "        #edgecolors = 'k',\n",
    "        markersize = 8,\n",
    "        capsize = 0,\n",
    "        linewidth = 1,\n",
    "        zorder = 3, ### m (sd) to front\n",
    "        )\n",
    "\n",
    "# format\n",
    "\n",
    "ax.set_xticks(x_positions)\n",
    "ax.set_xticklabels(\n",
    "    x_labels,\n",
    "    rotation = 45,\n",
    "    ha = 'right',\n",
    "    fontsize = 9,\n",
    "    )\n",
    "\n",
    "# transpose y-axis labels\n",
    "\n",
    "# hide left y-axis ticks and labels\n",
    "\n",
    "ax.tick_params(\n",
    "    axis = 'y',\n",
    "    left = False,\n",
    "    labelleft = False,\n",
    "    )\n",
    "\n",
    "# create a twin y-axis on the right\n",
    "\n",
    "ax_right = ax.secondary_yaxis('right')\n",
    "ax_right.set_yticks(list(y_labels.keys()))\n",
    "ax_right.set_yticklabels(list(y_labels.values()))\n",
    "\n",
    "ax.set_title(\n",
    "    \"I found NOLA Gem to be...\",\n",
    "    fontsize = 10,\n",
    "    )\n",
    "\n",
    "ax.grid(False)\n",
    "plt.tight_layout()\n",
    "\n",
    "# save\n",
    "\n",
    "plt.savefig(\n",
    "    'int1_int5_high_res_scatter.png',\n",
    "    dpi = 300,\n",
    "    )\n",
    "\n",
    "# display\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqc5gKR0Rfot",
    "tags": []
   },
   "source": [
    "##### **Fig. 2a.** NOLA Gem acceptability: perceived helpfulness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n9DVwG1e1Xg1"
   },
   "outputs": [],
   "source": [
    "# aesthetics\n",
    "\n",
    "sns.set_style(\n",
    "    style = 'whitegrid',\n",
    "    rc = None,\n",
    "    )\n",
    "\n",
    "int_cols = [\n",
    "    'intervention6_rev',\n",
    "    'intervention7_rev',\n",
    "    'intervention8_rev',\n",
    "    'intervention9_rev',\n",
    "    'intervention10_rev',\n",
    "    ]\n",
    "\n",
    "# customize x-axis labels\n",
    "\n",
    "x_positions = range(len(int_cols))\n",
    "x_labels = [\n",
    "    'reducing distress',\n",
    "    'improving mood',\n",
    "    'facilitating coping',\n",
    "    'changing habits',\n",
    "    'learning new skills',\n",
    "    ]\n",
    "\n",
    "# customize y-axis labels\n",
    "\n",
    "y_labels = {\n",
    "    1: 'disagree',\n",
    "    2: 'somewhat\\ndisagree',\n",
    "    3: 'somewhat\\nagree',\n",
    "    4: 'agree',\n",
    "    }\n",
    "\n",
    "# uniform color: ng_blue\n",
    "\n",
    "ng_blue = '#7eb0d5'\n",
    "\n",
    "# plot\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8, 5))\n",
    "\n",
    "# loop over int1-int5\n",
    "\n",
    "for i, col in enumerate(int_cols):\n",
    "    y_vals = d[col].astype(float) ### coerce numeric\n",
    "    x_jittered = np.random.normal(\n",
    "        loc = i,\n",
    "        scale = 0.16, ### inject jitter\n",
    "        size = len(y_vals),\n",
    "        )\n",
    "\n",
    "    # color = custom_colors[col]\n",
    "\n",
    "    ax.scatter(\n",
    "        x_jittered,\n",
    "        y_vals,\n",
    "        alpha = 0.6,\n",
    "        s = 35,\n",
    "        color = ng_blue,\n",
    "        #edgecolors = 'k',\n",
    "        linewidths = 0.5,\n",
    "        label = col,\n",
    "        )\n",
    "\n",
    "    # overlay m (sd) by int*\n",
    "\n",
    "    mean_val = y_vals.mean()\n",
    "    std_val = y_vals.std()\n",
    "\n",
    "    ax.errorbar(\n",
    "        i,\n",
    "        mean_val,\n",
    "        yerr = std_val,\n",
    "        fmt = 'D',\n",
    "        color = ng_blue,\n",
    "        #edgecolors = 'k',\n",
    "        markersize = 8,\n",
    "        capsize = 0,\n",
    "        linewidth = 1,\n",
    "        zorder = 3, ### m (sd) to front\n",
    "        )\n",
    "\n",
    "# format\n",
    "\n",
    "ax.set_xticks(x_positions)\n",
    "ax.set_xticklabels(\n",
    "    x_labels,\n",
    "    rotation = 45,\n",
    "    ha = 'right',\n",
    "    fontsize = 9,\n",
    "    )\n",
    "\n",
    "# hide left y-axis ticks and labels\n",
    "\n",
    "ax.tick_params(\n",
    "    axis = 'y',\n",
    "    left = False,\n",
    "    labelleft = False,\n",
    "    )\n",
    "\n",
    "# create a twin y-axis on the right\n",
    "\n",
    "ax_right = ax.secondary_yaxis('right')\n",
    "ax_right.set_yticks(list(y_labels.keys()))\n",
    "ax_right.set_yticklabels(list(y_labels.values()))\n",
    "\n",
    "ax.set_title(\n",
    "    \"($a.$) I found NOLA Gem to be helpful in...\",\n",
    "    fontsize = 10,\n",
    "    )\n",
    "\n",
    "ax.grid(False)\n",
    "plt.tight_layout()\n",
    "\n",
    "# save\n",
    "\n",
    "plt.savefig(\n",
    "    'int6_int10_high_res_scatter.png',\n",
    "    dpi = 300,\n",
    "    )\n",
    "\n",
    "# display\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drYSb9WIT-fv",
    "tags": []
   },
   "source": [
    "##### **Fig. 2b.** NOLA Gem acceptability: perceived helpfulness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1oCbPcu7SpXK"
   },
   "outputs": [],
   "source": [
    "# aesthetics\n",
    "\n",
    "sns.set_style(\n",
    "    style = 'whitegrid',\n",
    "    rc = None,\n",
    "    )\n",
    "\n",
    "int_cols = [\n",
    "    'intervention11_rev',\n",
    "    'intervention12_rev',\n",
    "    'intervention13_rev',\n",
    "    'intervention14_rev',\n",
    "    ]\n",
    "\n",
    "# customize x-axis labels\n",
    "\n",
    "x_positions = range(len(int_cols))\n",
    "x_labels = [\n",
    "    'educational sessions',\n",
    "    'skills practice',\n",
    "    'geofencing alerts',\n",
    "    'daily diary suggested skills',\n",
    "    ]\n",
    "\n",
    "# customize y-axis labels\n",
    "\n",
    "y_labels = {\n",
    "    1: 'not very helpful',\n",
    "    2: 'a little\\nhelpful',\n",
    "    3: 'somewhat\\nhelpful',\n",
    "    4: 'very helpful',\n",
    "    }\n",
    "\n",
    "# uniform color: ng_blue\n",
    "\n",
    "ng_blue = '#7eb0d5'\n",
    "\n",
    "# plot\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8, 5))\n",
    "\n",
    "# loop over int1-int5\n",
    "\n",
    "for i, col in enumerate(int_cols):\n",
    "    y_vals = d[col].astype(float).replace(5, np.nan).dropna() ### coerce numeric, convert '5' to NaN and drop\n",
    "    x_jittered = np.random.normal(\n",
    "        loc = i,\n",
    "        scale = 0.16, ### inject jitter\n",
    "        size = len(y_vals),\n",
    "        )\n",
    "\n",
    "    # color = custom_colors[col]\n",
    "\n",
    "    ax.scatter(\n",
    "        x_jittered,\n",
    "        y_vals,\n",
    "        alpha = 0.6,\n",
    "        s = 35,\n",
    "        color = ng_blue,\n",
    "        #edgecolors = 'k',\n",
    "        linewidths = 0.5,\n",
    "        label = col,\n",
    "        )\n",
    "\n",
    "    # overlay m (sd) by int*\n",
    "\n",
    "    mean_val = y_vals.mean()\n",
    "    std_val = y_vals.std()\n",
    "\n",
    "    ax.errorbar(\n",
    "        i,\n",
    "        mean_val,\n",
    "        yerr = std_val,\n",
    "        fmt = 'D',\n",
    "        color = ng_blue,\n",
    "        #edgecolors = 'k',\n",
    "        markersize = 8,\n",
    "        capsize = 0,\n",
    "        linewidth = 1,\n",
    "        zorder = 3, ### m (sd) to front\n",
    "        )\n",
    "\n",
    "# format\n",
    "\n",
    "ax.set_xticks(x_positions)\n",
    "ax.set_xticklabels(\n",
    "    x_labels,\n",
    "    rotation = 45,\n",
    "    ha = 'right',\n",
    "    fontsize = 9,\n",
    "    )\n",
    "\n",
    "# hide left y-axis ticks and labels\n",
    "\n",
    "ax.tick_params(\n",
    "    axis = 'y',\n",
    "    left = False,\n",
    "    labelleft = False,\n",
    "    )\n",
    "\n",
    "# create a twin y-axis on the right\n",
    "\n",
    "ax_right = ax.secondary_yaxis('right')\n",
    "ax_right.set_yticks(list(y_labels.keys()))\n",
    "ax_right.set_yticklabels(list(y_labels.values()))\n",
    "\n",
    "ax.set_title(\n",
    "    \"($b.$) I found the NOLA Gem __________ features to be...$^a$\",\n",
    "    fontsize = 10,\n",
    "    )\n",
    "\n",
    "ax.grid(False)\n",
    "plt.tight_layout()\n",
    "\n",
    "# save\n",
    "\n",
    "plt.savefig(\n",
    "    'int11_int14_high_res_scatter.png',\n",
    "    dpi = 300,\n",
    "    )\n",
    "\n",
    "# display\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rkx4GpWzzwsq",
    "tags": []
   },
   "source": [
    "### 4. Code\n",
    "Applies on-device 8B Llama-human synergistic deductive coding\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVLRLHlT0yMt"
   },
   "source": [
    "#### Transform qual-ready df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = 'C:/Users/sskee/OneDrive/Documents/02_tulane/01_research/nola_gem/dissem/skeen,etal_acceptability/data'\n",
    "os.chdir(wd)\n",
    "%pwd\n",
    "\n",
    "d = pd.read_csv(\n",
    "    'nola_gem_acceptability_no_loc_tx.csv',\n",
    "    index_col = [0],\n",
    "    )\n",
    "\n",
    "d.info()\n",
    "d.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V68aDTR51jAR"
   },
   "outputs": [],
   "source": [
    "#cols = d.columns.tolist()\n",
    "#print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "loTG5glMUaO7"
   },
   "outputs": [],
   "source": [
    "d_qual = d[[\n",
    "    'open1',\n",
    "    'open2',\n",
    "    'open3',\n",
    "    'open4',\n",
    "    ]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AvjqrLzk1zys"
   },
   "outputs": [],
   "source": [
    "# insert qual code / rationale cells\n",
    "\n",
    "d_qual[[\n",
    "    'fctn_sjs', 'fctn_rtnl_sjs', 'lgth_sjs', 'lgth_rtnl_sjs',\n",
    "    'tmng_sjs', 'tmng_rtnl_sjs', 'attn_sjs', 'attn_rtnl_sjs',\n",
    "    'gltc_sjs', 'gltc_rtnl_sjs', 'prfc_sjs', 'prfc_rtnl_sjs',\n",
    "    'ftrs_sjs', 'ftrs_rtnl_sjs', 'strs_sjs', 'strs_rtnl_sjs',\n",
    "    ]] = ' '\n",
    "\n",
    "d_qual.info()\n",
    "d_qual.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_Y_hMMjFZy2"
   },
   "outputs": [],
   "source": [
    "# transform: long qual format for human/LLM coding\n",
    "\n",
    "# list qual columns\n",
    "\n",
    "qual_cols = [\n",
    "    'open1',\n",
    "    'open2',\n",
    "    'open3',\n",
    "    'open4',\n",
    "    ]\n",
    "\n",
    "# dedicate provisional df per qual column\n",
    "\n",
    "for col in qual_cols:\n",
    "    keep_cols = [c for c in d_qual.columns if c not in qual_cols or c == col]\n",
    "    globals()[f'd_qual_{col}'] = d_qual[keep_cols].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p-47ZRIMFZ4c"
   },
   "outputs": [],
   "source": [
    "# specify 'item' col for long df\n",
    "\n",
    "d_qual_open1['item'] = 'open1'\n",
    "d_qual_open2['item'] = 'open2'\n",
    "d_qual_open3['item'] = 'open3'\n",
    "d_qual_open4['item'] = 'open4'\n",
    "\n",
    "# rename all qual cols: 'text'\n",
    "\n",
    "d_qual_open1 = d_qual_open1.rename(\n",
    "    columns = {'open1': 'text'},\n",
    "    )\n",
    "d_qual_open2 = d_qual_open2.rename(\n",
    "    columns = {'open2': 'text'},\n",
    "    )\n",
    "d_qual_open3 = d_qual_open3.rename(\n",
    "    columns = {'open3': 'text'},\n",
    "    )\n",
    "d_qual_open4 = d_qual_open4.rename(\n",
    "    columns = {'open4': 'text'},\n",
    "    )\n",
    "\n",
    "# concatenate: long df\n",
    "\n",
    "d_qual_long = pd.concat(\n",
    "    [d_qual_open1, d_qual_open2,\n",
    "     d_qual_open3, d_qual_open4],\n",
    "    axis = 0,\n",
    "    ignore_index = False,\n",
    "    )\n",
    "\n",
    "# reorder\n",
    "\n",
    "d_qual_long = d_qual_long.reindex(columns = [\n",
    "    'item', 'text',\n",
    "    'fctn_sjs',\t'fctn_rtnl_sjs', 'lgth_sjs', 'lgth_rtnl_sjs', 'tmng_sjs', 'tmng_rtnl_sjs',\n",
    "    'attn_sjs',\t'attn_rtnl_sjs', 'gltc_sjs', 'gltc_rtnl_sjs', 'prfc_sjs', 'prfc_rtnl_sjs',\n",
    "    'ftrs_sjs',\t'ftrs_rtnl_sjs', 'strs_sjs', 'strs_rtnl_sjs',\n",
    "    ])\n",
    "\n",
    "d_qual_long.info()\n",
    "d_qual_long.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WgOS6uZD5hkY"
   },
   "outputs": [],
   "source": [
    "#%pwd\n",
    "%cd ../../inputs/data\n",
    "\n",
    "# export\n",
    "\n",
    "d_qual_long.to_excel('nola_gem_acceptability_qual.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgbuKac52ALs"
   },
   "source": [
    "#### Formulate deductive coding prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tj4Ruxvd5PCk"
   },
   "outputs": [],
   "source": [
    "role = '''\n",
    "    You are tasked with applying pre-defined qualitative codes to segments of text excerpted \n",
    "    from interviews with users of a mental health app for people living with HIV. \n",
    "    \n",
    "    The app included text-messaged surveys multiple times a day, as well as self-directed sessions \n",
    "    on mindfulness meditation, trauma psychoeducation (etc.), and momentary prompts to engage with \n",
    "    brief coping skills training as needed throughout the day. \n",
    "    \n",
    "    You will be provided a definition, instructions, and \n",
    "    key exemplars of text to guide your coding decisions.\n",
    "    '''\n",
    "\n",
    "text = '''\n",
    "    Text:\n",
    "    {text}\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q646xsta2gAD"
   },
   "source": [
    "##### _EMA questionnaire length_ (alias: `lgth`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xiqPgZgM3tzw",
    "outputId": "c065f9c9-8a35-43a9-aa29-e1d0a59a5492",
    "tags": []
   },
   "outputs": [],
   "source": [
    "code = 'EMA questionnaire length'\n",
    "alias = 'lgth'\n",
    "code_def = '''\n",
    "    Describes frustration with text message survey, daily diary, or questionnaire (all refer to the same measure) length. \n",
    "    These frustrations can include aspects such as the number of questions, or the amount of time allotted to complete the \n",
    "    questionnaire, or the nature of the questions administered to participants.\n",
    "    \n",
    "    Frustrations must be directly attributable to the duration of the survey or the time it takes to respond to, specifically\n",
    "    and explicitly. Vague dissatisfaction with the survey does not warrant a 'lgth' = 1. \n",
    "    '''\n",
    "code_ex = '''\n",
    "    - \"The questions they texted you, there was just too many…the questions seemed to ask the same thing over and again\"\n",
    "    - \"The number of questions was too many. It's inconvenient especially if you have typical work and family responsibilities\"\n",
    "    - \"There are so many questions on the survey and they don't give you enough time to finish them all\"\n",
    "    '''\n",
    "\n",
    "# ----------------------------------------------------------------------------------------- #\n",
    "definition = f'''\n",
    "    Definition of \"{code}\": {code_def}.\n",
    "    '''\n",
    "\n",
    "instruction = f'''\n",
    "    You will be provided with a piece of text. For each piece of text:\n",
    "    - If it meets the definition of \"{code},\" output {alias}_llm as \"1\".\n",
    "    - Otherwise, output {alias}_llm as \"0\".\n",
    "    - Also provide a short explanation in exactly two sentences, stored in {alias}_expl.\n",
    "\n",
    "    Please respond in valid JSON with keys \"{alias}_llm\" and \"{alias}_expl\" only.\n",
    "    '''\n",
    "\n",
    "example = f'''\n",
    "    Below are human-validated examples of \"{code}\"\n",
    "\n",
    "    - \"{code_ex}\"\n",
    "    '''\n",
    "# ----------------------------------------------------------------------------------------- #\n",
    "\n",
    "# concatenate prompt as f-string\n",
    "\n",
    "lgth_prompt = f'{role}{definition}{instruction}{text}{example}'\n",
    "print(lgth_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbOPu9Mi21Kk"
   },
   "source": [
    "##### _EMA prompt timing_ (alias: `tmng`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePbg6rXo54Fe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "code = 'EMA prompt timing'\n",
    "alias = 'tmng'\n",
    "code_def = '''\n",
    "    Describes frustration with text message survey, daily diary, or questionnaire (all refer to the same measure) timing during the day. \n",
    "    These frustrations can include the questionnaires being sent too early, or too late, or being difficult to fit in due to obligations\n",
    "    such as a full-time job.\n",
    "    \n",
    "    Frustrations must be directly attributable to the timing of the survey delivery, specifically\n",
    "    and explicitly. Vague dissatisfaction with the survey does not warrant a 'tmng' = 1. \n",
    "    '''\n",
    "code_ex = '''\n",
    "    - \"I couldn’t always answer the surveys when they sent them during the day…because I’ve got a full-time job\"\n",
    "    - \"The timing was all wrong. The diaries should come in earlier or later in the day so I can answer without disrupting my schedule\"\n",
    "    - \"The time periods for the text message surveys was unworkable for me. Timing would be better if you could choose yourself\"\n",
    "    '''\n",
    "\n",
    "# ----------------------------------------------------------------------------------------- #\n",
    "definition = f'''\n",
    "    Definition of \"{code}\": {code_def}.\n",
    "    '''\n",
    "\n",
    "instruction = f'''\n",
    "    You will be provided with a piece of text. For each piece of text:\n",
    "    - If it meets the definition of \"{code},\" output {alias}_llm as \"1\".\n",
    "    - Otherwise, output {alias}_llm as \"0\".\n",
    "    - Also provide a short explanation in exactly two sentences, stored in {alias}_expl.\n",
    "\n",
    "    Please respond in valid JSON with keys \"{alias}_llm\" and \"{alias}_expl\" only.\n",
    "    '''\n",
    "\n",
    "example = f'''\n",
    "    Below are human-validated examples of \"{code}\"\n",
    "\n",
    "    - \"{code_ex}\"\n",
    "    '''\n",
    "# ----------------------------------------------------------------------------------------- #\n",
    "\n",
    "# concatenate prompt as f-string\n",
    "\n",
    "tmng_prompt = f'{role}{definition}{instruction}{text}{example}'\n",
    "print(tmng_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_d1TI-l3Eyk"
   },
   "source": [
    "##### _EMA item attunement_ (alias: `attn`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzkZLW9U3Dpn"
   },
   "outputs": [],
   "source": [
    "code = 'EMA item attunement'\n",
    "alias = 'attn'\n",
    "code_def = '''\n",
    "    Recounts opinions that text message survey, daily diary, or questionnaire (all refer to the same measure) were inappropriate to \n",
    "    or incongruent with app users’ lived experience, including recommendations for more appropriate questions or question wording.\n",
    "    \n",
    "    The text must refer to the mismatch between item wording and the user's life experience, explicitly and specifically, to\n",
    "    warrant a 'attn' = 1. Mentions of formatting issues such as the survey length or timing, alone, do _not_ warrant a 'attn' = 1\n",
    "    '''\n",
    "code_ex = '''\n",
    "    - \"Not all of the questions made sense to me, or really fit into how I thik about my life\"\n",
    "    - \"The questions felt too vague. I wasn't sure what they meant\"\n",
    "    - \"More questions on prayer and surivival, less about drugs and violence\"\n",
    "    '''\n",
    "\n",
    "# ----------------------------------------------------------------------------------------- #\n",
    "definition = f'''\n",
    "    Definition of \"{code}\": {code_def}.\n",
    "    '''\n",
    "\n",
    "instruction = f'''\n",
    "    You will be provided with a piece of text. For each piece of text:\n",
    "    - If it meets the definition of \"{code},\" output {alias}_llm as \"1\".\n",
    "    - Otherwise, output {alias}_llm as \"0\".\n",
    "    - Also provide a short explanation in exactly two sentences, stored in {alias}_expl.\n",
    "\n",
    "    Please respond in valid JSON with keys \"{alias}_llm\" and \"{alias}_expl\" only.\n",
    "    '''\n",
    "\n",
    "example = f'''\n",
    "    Below are human-validated examples of \"{code}\"\n",
    "\n",
    "    - \"{code_ex}\"\n",
    "    '''\n",
    "# ----------------------------------------------------------------------------------------- #\n",
    "\n",
    "# concatenate prompt as f-string\n",
    "\n",
    "attn_prompt = f'{role}{definition}{instruction}{text}{example}'\n",
    "print(attn_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7yBt-s_3aLD"
   },
   "source": [
    "##### “_Gotta get those glitches fixed_ [in vivo]” (alias: `gltc`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nTzFuGaG3Dte"
   },
   "outputs": [],
   "source": [
    "code = 'Glitch fixes'\n",
    "alias = 'gltc'\n",
    "code_def = '''\n",
    "    Describes any app (including text message delivery) malfunctions.\n",
    "    '''\n",
    "code_ex = '''\n",
    "    - \"It just froze while I was answering my survey one day\"\n",
    "    - \"I feel like it could be great but it felt too glitchy for now\"\n",
    "    - \"The glitches. I just couldn't deal with the glitches\"\n",
    "    '''\n",
    "\n",
    "# ----------------------------------------------------------------------------------------- #\n",
    "definition = f'''\n",
    "    Definition of \"{code}\": {code_def}.\n",
    "    '''\n",
    "\n",
    "instruction = f'''\n",
    "    You will be provided with a piece of text. For each piece of text:\n",
    "    - If it meets the definition of \"{code},\" output {alias}_llm as \"1\".\n",
    "    - Otherwise, output {alias}_llm as \"0\".\n",
    "    - Also provide a short explanation in exactly two sentences, stored in {alias}_expl.\n",
    "\n",
    "    Please respond in valid JSON with keys \"{alias}_llm\" and \"{alias}_expl\" only.\n",
    "    '''\n",
    "\n",
    "example = f'''\n",
    "    Below are human-validated examples of \"{code}\"\n",
    "\n",
    "    - \"{code_ex}\"\n",
    "    '''\n",
    "# ----------------------------------------------------------------------------------------- #\n",
    "\n",
    "# concatenate prompt as f-string\n",
    "\n",
    "gltc_prompt = f'{role}{definition}{instruction}{text}{example}'\n",
    "print(gltc_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6pDbOL73zTT"
   },
   "source": [
    "##### \"_This app is perfect_ [in vivo]\" (alias: `prfc`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i081-ElA3Dw-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "code = 'Perfection'\n",
    "alias = 'prfc'\n",
    "code_def = '''\n",
    "    Captures all responses that express satisfaction with the pilot app or reject any suggestions for updated features.\n",
    "    \n",
    "    Positive sentiment towards the app, alone, does _not_ warrant a 'prfc' = 1; statements that nothing needs to be changed \n",
    "\tmust be specific and explicit to warrant a 'prfc' = 1.\n",
    "    '''\n",
    "code_ex = '''\n",
    "    - \"I don't think you need to put in any changes at all\"\n",
    "    - \"There was nothing I didn't like\"\n",
    "    - \"I liked it all\"\n",
    "    '''\n",
    "\n",
    "# ----------------------------------------------------------------------------------------- #\n",
    "definition = f'''\n",
    "    Definition of \"{code}\": {code_def}.\n",
    "    '''\n",
    "\n",
    "instruction = f'''\n",
    "    You will be provided with a piece of text. For each piece of text:\n",
    "    - If it meets the definition of \"{code},\" output {alias}_llm as \"1\".\n",
    "    - Otherwise, output {alias}_llm as \"0\".\n",
    "    - Also provide a short explanation in exactly two sentences, stored in {alias}_expl.\n",
    "\n",
    "    Please respond in valid JSON with keys \"{alias}_llm\" and \"{alias}_expl\" only.\n",
    "    '''\n",
    "\n",
    "example = f'''\n",
    "    Below are human-validated examples of \"{code}\"\n",
    "\n",
    "    - \"{code_ex}\"\n",
    "    '''\n",
    "# ----------------------------------------------------------------------------------------- #\n",
    "\n",
    "# concatenate prompt as f-string\n",
    "\n",
    "prfc_prompt = f'{role}{definition}{instruction}{text}{example}'\n",
    "print(prfc_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tweaks and fresh features (alias: `ftrs`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = 'Tweaks and fresh features'\n",
    "alias = 'ftrs'\n",
    "code_def = '''\n",
    "    Captures all specific recommendations for alterations to existing app features or expressed wishes for \n",
    "    entirely new features to be added to the app.\n",
    "    \n",
    "    New features must be suggested clearly and specifically to warrant a 'ftrs' = 1; dissatisfaction with \n",
    "    existing features alone does _not_ warrant a 'ftrs' = 1.\n",
    "    '''\n",
    "code_ex = '''\n",
    "    - \"More coping skill should be added\"\n",
    "    - \"I wish it had less text, more multimedia\"\n",
    "    - \"The questions should vbe more personalized use to user to better fit our needs\"\n",
    "    '''\n",
    "\n",
    "# ----------------------------------------------------------------------------------------- #\n",
    "definition = f'''\n",
    "    Definition of \"{code}\": {code_def}.\n",
    "    '''\n",
    "\n",
    "instruction = f'''\n",
    "    You will be provided with a piece of text. For each piece of text:\n",
    "    - If it meets the definition of \"{code},\" output {alias}_llm as \"1\".\n",
    "    - Otherwise, output {alias}_llm as \"0\".\n",
    "    - Also provide a short explanation in exactly two sentences, stored in {alias}_expl.\n",
    "\n",
    "    Please respond in valid JSON with keys \"{alias}_llm\" and \"{alias}_expl\" only.\n",
    "    '''\n",
    "\n",
    "example = f'''\n",
    "    Below are human-validated examples of \"{code}\"\n",
    "\n",
    "    - \"{code_ex}\"\n",
    "    '''\n",
    "# ----------------------------------------------------------------------------------------- #\n",
    "\n",
    "# concatenate prompt as f-string\n",
    "\n",
    "ftrs_prompt = f'{role}{definition}{instruction}{text}{example}'\n",
    "print(ftrs_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSydTIWx431o"
   },
   "source": [
    "##### “_You seem a little stressed…it will tell you what you can do_ [in vivo]” (alias: `strs`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = 'JITAI recognition'\n",
    "alias = 'strs'\n",
    "code_def = '''\n",
    "    Captures any expression of appreciation for the app's features being offered when needed throughout the day,\n",
    "    or tailored to particular stressors reported by the user.\n",
    "    \n",
    "    Positive sentiment towards the app, alone, does _not_ warrant a 'strs' = 1; descriptions of the app being uniquely \n",
    "    responsive to users' needs must be specific and explicit to warrant a 'strs' = 1.\n",
    "    '''\n",
    "code_ex = '''\n",
    "    - \"I liked how different meditations were offered for different stresses you mention throughout the day\"\n",
    "    - \"The daily check-ins and quick follow ups to try and help you right when you most need it\"\n",
    "    - \"Learning all these new skills to stay calm and collected during the day\"\n",
    "    '''\n",
    "\n",
    "# ----------------------------------------------------------------------------------------- #\n",
    "definition = f'''\n",
    "    Definition of \"{code}\": {code_def}.\n",
    "    '''\n",
    "\n",
    "instruction = f'''\n",
    "    You will be provided with a piece of text. For each piece of text:\n",
    "    - If it meets the definition of \"{code},\" output {alias}_llm as \"1\".\n",
    "    - Otherwise, output {alias}_llm as \"0\".\n",
    "    - Also provide a short explanation in exactly two sentences, stored in {alias}_expl.\n",
    "\n",
    "    Please respond in valid JSON with keys \"{alias}_llm\" and \"{alias}_expl\" only.\n",
    "    '''\n",
    "\n",
    "example = f'''\n",
    "    Below are human-validated examples of \"{code}\"\n",
    "\n",
    "    - \"{code_ex}\"\n",
    "    '''\n",
    "# ----------------------------------------------------------------------------------------- #\n",
    "\n",
    "# concatenate prompt as f-string\n",
    "\n",
    "strs_prompt = f'{role}{definition}{instruction}{text}{example}'\n",
    "print(strs_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dual-code human-coded qual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BktZJOY03D0g"
   },
   "outputs": [],
   "source": [
    "#%cd /content/drive/My Drive/Colab/nola_gem_acceptability/inputs/data\n",
    "\n",
    "os.chdir('C:/Users/sskee/OneDrive/Documents/02_tulane/01_research/nola_gem/dissem/skeen,etal_acceptability/data/qual')\n",
    "\n",
    "d = pd.read_excel(\n",
    "    'nola_gem_acceptability_qual_sjs.xlsx',\n",
    "    index_col = [0],\n",
    "    )\n",
    "\n",
    "d.info()\n",
    "d.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cfyU-8J73D3T",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%%capture\n",
    "\n",
    "# locally hosted Ollama endpoint\n",
    "\n",
    "ollama_endpoint = 'http://localhost:11434/api/generate'\n",
    "\n",
    "# define aliases, prompts\n",
    "\n",
    "prompts = [\n",
    "    ('lgth', lgth_prompt), ### (alias, prompt_template)\n",
    "    ('tmng', tmng_prompt),\n",
    "    ('attn', attn_prompt),\n",
    "    ('gltc', gltc_prompt),\n",
    "    ('prfc', prfc_prompt),\n",
    "    ('ftrs', ftrs_prompt),\n",
    "    ('strs', strs_prompt),\n",
    "    ]\n",
    "\n",
    "# loop through each alias, prompt\n",
    "\n",
    "for alias, prompt_template in prompts:\n",
    "\n",
    "# apply code_texts_deductively_ollama over aliases, prompts, updated_df\n",
    "\n",
    "    d_coded = code_texts_deductively_ollama(\n",
    "        d,\n",
    "        alias = alias,\n",
    "        text_column = 'text',\n",
    "        endpoint_url = ollama_endpoint,\n",
    "        prompt_template = prompt_template,\n",
    "        model_name = 'gemma3:12B',\n",
    "        )\n",
    "\n",
    "d_coded.head(3)\n",
    "\n",
    "# export\n",
    "\n",
    "d_coded.to_excel('d_coded.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Cohen's $\\kappa$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/sskee/OneDrive/Documents/02_tulane/01_research/nola_gem/dissem/skeen,etal_acceptability/data/qual')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_excel(\n",
    "    'd_coded.xlsx',\n",
    "    index_col = [0],\n",
    "    )\n",
    "\n",
    "encodings_sjs = [\n",
    "    'lgth_sjs', \n",
    "    'tmng_sjs',\n",
    "    'attn_sjs',\n",
    "    'gltc_sjs',\n",
    "    'prfc_sjs',\n",
    "    'ftrs_sjs',\n",
    "    'strs_sjs',\n",
    "    ]\n",
    "\n",
    "# numeric conversion - coerce\n",
    "\n",
    "for e in encodings_sjs:\n",
    "    d[e] = pd.to_numeric(d[e], errors = 'coerce')\n",
    "\n",
    "# replace NaN w/ 0    \n",
    "    \n",
    "d[encodings_sjs] = d[encodings_sjs].fillna(0)\n",
    "\n",
    "# inspect\n",
    "\n",
    "d.info()\n",
    "d.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define kappa fx\n",
    "\n",
    "def calculate_kappa(d, col1, col2):\n",
    "    return cohen_kappa_score(d[col1], d[col2])\n",
    "\n",
    "col_pairs = [\n",
    "    ('lgth_sjs', 'lgth_llm'), \n",
    "    ('tmng_sjs', 'tmng_llm'),\n",
    "    ('attn_sjs', 'attn_llm'),\n",
    "    ('gltc_sjs', 'gltc_llm'),\n",
    "    ('prfc_sjs', 'prfc_llm'),\n",
    "    ('ftrs_sjs', 'ftrs_llm'),\n",
    "    ('strs_sjs', 'strs_llm'),\n",
    "    ]\n",
    "\n",
    "# initialize dict\n",
    "\n",
    "kappa_results = {}\n",
    "\n",
    "# % agreement loop\n",
    "\n",
    "def calculate_percent_agreement(df, col_pairs):\n",
    "    results = {}\n",
    "    for col1, col2 in col_pairs:\n",
    "        agreement = df[col1] == df[col2]\n",
    "        percent_agreement = (agreement.sum() / len(df)) * 100\n",
    "        results[f\"{col1} & {col2}\"] = percent_agreement\n",
    "    return results\n",
    "\n",
    "percent_agreement_results = calculate_percent_agreement(d, col_pairs)\n",
    "\n",
    "for pair, percent in percent_agreement_results.items():\n",
    "    print(f\"Percent agreement for {pair}: {percent:.2f}%\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# kappa loop\n",
    "\n",
    "for col1, col2 in col_pairs:\n",
    "    kappa = calculate_kappa(d, col1, col2)\n",
    "    kappa_results[f'{col1} and {col2}'] = kappa\n",
    "\n",
    "for pair, kappa in kappa_results.items():\n",
    "    print(f\"Cohen's Kappa for {pair}: {kappa:.2f}\")\n",
    "    \n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _drop_ `ftrs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d.drop(columns = [col for col in d.columns if col.startswith(('ftrs_', 'fctn_'))]) ### drop 'fctn_' cols - superordinate theme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flag disagreements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(d.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag disagreements fx\n",
    "\n",
    "def encode_disagreements(row):\n",
    "    return 1 if row[0] != row[1] else 0\n",
    "\n",
    "col_dis = [\n",
    "    ('lgth_sjs', 'lgth_llm', 'lgth_dis'), \n",
    "    ('tmng_sjs', 'tmng_llm', 'tmng_dis'),\n",
    "    ('attn_sjs', 'attn_llm', 'attn_dis'),\n",
    "    ('gltc_sjs', 'gltc_llm', 'gltc_dis'),\n",
    "    ('prfc_sjs', 'prfc_llm', 'prfc_dis'),\n",
    "    ('strs_sjs', 'strs_llm', 'strs_dis'),\n",
    "    ]\n",
    "\n",
    "for col1, col2, dis_col in col_dis:\n",
    "    d[dis_col] = d[[col1, col2]].apply(\n",
    "        encode_disagreements,\n",
    "        axis = 1,\n",
    "        )\n",
    "\n",
    "# obfuscate human vs. Gemma encodings    \n",
    "    \n",
    "# Example: rename _sjs to _a and _llm to _b\n",
    "\n",
    "#d.columns = [\n",
    "#    col.replace('_sjs', '_a').replace('_llm', '_b') if col.endswith(('_sjs', '_llm')) else col\n",
    "#    for col in d.columns\n",
    "#    ]\n",
    "\n",
    "# reorder for ST interpretability \n",
    "\n",
    "d = d[[\n",
    "    'item', 'text', \n",
    "    'lgth_sjs', 'lgth_rtnl_sjs', 'lgth_llm', 'lgth_expl','lgth_dis', \n",
    "    'tmng_sjs', 'tmng_rtnl_sjs', 'tmng_llm', 'tmng_expl', 'tmng_dis', \n",
    "    'attn_sjs', 'attn_rtnl_sjs', 'attn_llm', 'attn_expl', 'attn_dis', \n",
    "    'gltc_sjs', 'gltc_rtnl_sjs','gltc_llm', 'gltc_expl', 'gltc_dis', \n",
    "    'prfc_sjs', 'prfc_rtnl_sjs','prfc_llm', 'prfc_expl', 'prfc_dis', \n",
    "    'strs_sjs', 'strs_rtnl_sjs', 'strs_llm', 'strs_expl', 'strs_dis',\n",
    "    ]]\n",
    "\n",
    "# inspect\n",
    "\n",
    "d.info()\n",
    "d.head(3)    \n",
    "        \n",
    "# export\n",
    "\n",
    "d.to_excel(f'd_coded_icr.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> End of nola_gem_acceptability.ipynb"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "RS6CgqU2RO4r",
    "qqc5gKR0Rfot",
    "drYSb9WIT-fv"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
