{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzgKP4GEalxQ"
   },
   "source": [
    "## Acceptability, feasibility, and user experiences of ðŸ’Ž NOLA Gem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRMega9UavJZ"
   },
   "source": [
    "_WIP - PRESENTLY AT R&R_\n",
    "\n",
    "_Viz. and local LLMâ€“assisted deductive qual coding scripts for pilot trial (N = 32) analyses of acceptability, feasibility, and user experience with [ðŸ’Ž NOLA Gem](https://www.researchprotocols.org/2023/1/e47151/authors): a geospatially customizable culturally tailored JITAI for violence-affected people living with HIV._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5O-o5plbL4_"
   },
   "source": [
    "> `nola_gem_acceptability.ipynb`<br>\n",
    "> Simone J. Skeen x Claude Code (01-28-2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrrkzGPlbaaQ"
   },
   "source": [
    "### 1. Prepare\n",
    "Installs, imports, requisite packages; customizes outputs.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gu7SLsTr09uq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%pip install ollama\n",
    "#%pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atjpV2bUbhzM"
   },
   "outputs": [],
   "source": [
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from matplotlib import cm\n",
    "from matplotlib.lines import Line2D\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "for c in (FutureWarning, UserWarning):\n",
    "    warnings.simplefilter(action = 'ignore', category = c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJHePSXTcFyM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# path configuration\n",
    "\n",
    "### update ROOT to match your local directory structure\n",
    "\n",
    "ROOT = Path('.')  # project root\n",
    "\n",
    "DATA_DIR = ROOT / 'data'\n",
    "DATA_PARADATA = DATA_DIR / 'paradata'\n",
    "DATA_QUAL = DATA_DIR / 'qual'\n",
    "FIGURES_DIR = ROOT / 'outputs' / 'figures'\n",
    "\n",
    "### optional: custom font (uncomment and update path if needed)\n",
    "\n",
    "#fm.fontManager.addfont('/path/to/Arial.ttf')\n",
    "#plt.rcParams['font.family'] = 'Arial'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write\n",
    "Defines `qualitative.py` module.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile qualitative.py\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def build_prompt(code, alias, code_def, code_ex):\n",
    "    '''\n",
    "    Constructs a deductive coding prompt for LLM-based qualitative analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    code : str\n",
    "        Full name of the qualitative code (e.g., 'JITAI recognition').\n",
    "    alias : str\n",
    "        Short alias for column naming (e.g., 'strs').\n",
    "    code_def : str\n",
    "        Definition of what the code captures, including inclusion/exclusion criteria.\n",
    "    code_ex : str\n",
    "        Human-validated examples of text that should receive this code.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Complete prompt string ready for LLM consumption.\n",
    "    '''\n",
    "    \n",
    "    role = '''\n",
    "    You are tasked with applying pre-defined qualitative codes to segments of text excerpted \n",
    "    from interviews with users of a mental health app for people living with HIV. \n",
    "    \n",
    "    The app included text-messaged surveys multiple times a day, as well as self-directed sessions \n",
    "    on mindfulness meditation, trauma psychoeducation (etc.), and momentary prompts to engage with \n",
    "    brief coping skills training as needed throughout the day. \n",
    "    \n",
    "    You will be provided a definition, instructions, and \n",
    "    key exemplars of text to guide your coding decisions.\n",
    "    '''\n",
    "\n",
    "    text = '''\n",
    "    ---\n",
    "    Text to classify:\n",
    "    \"{text}\"\n",
    "    ---\n",
    "    '''\n",
    "    \n",
    "    definition = f'''\n",
    "    Definition of \"{code}\": {code_def}.\n",
    "    '''\n",
    "    \n",
    "    instruction = f'''\n",
    "    You will be provided with a piece of text. For each piece of text:\n",
    "    - If it meets the definition of \"{code},\" output {alias}_llm as \"1\".\n",
    "    - Otherwise, output {alias}_llm as \"0\".\n",
    "    - Also provide a short explanation in exactly two sentences, stored in {alias}_expl.\n",
    "\n",
    "    Please respond in valid JSON with keys \"{alias}_llm\" and \"{alias}_expl\" only.\n",
    "    '''\n",
    "    \n",
    "    example = f'''\n",
    "    Below are human-validated examples of \"{code}\"\n",
    "\n",
    "    - \"{code_ex}\"\n",
    "    '''\n",
    "    \n",
    "    return f'{role}{definition}{instruction}{text}{example}'\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "def code_texts_deductively_ollama(df, alias, text_column, endpoint_url, prompt_template, model_name):\n",
    "    '''\n",
    "    Classifies each row of 'text' column in provided df in accord with human-specified prompt,\n",
    "    includes chain-of-thought reasoning, returning explanations for classification decision.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        df containing the text to classify.\n",
    "    alias : str\n",
    "        alias (for brevity) of the qualitative code to be applied.\n",
    "    text_column : str\n",
    "        column name in df containing the text to be analyzed.\n",
    "    endpoint_url : str\n",
    "        URL where locally hosted LLM runs.\n",
    "    prompt_template : str\n",
    "        prompt text with a placeholder (e.g. '{text}') where the row's text will be inserted.\n",
    "    model_name : str\n",
    "        model tasked with qualitative deductive coding.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "   df : pd.DataFrame\n",
    "        The original df with two new columns per deductive code: '{alias}_llm' (either \"0\" or \"1\")\n",
    "        and '{alias}_expl' (the chain-of-thought explanation)\n",
    "    '''\n",
    "\n",
    "    label_column = f'{alias}_llm'\n",
    "    explanation_column = f'{alias}_expl'\n",
    "\n",
    "    # insert cols - necessary for .update() downstream\n",
    "    \n",
    "    df[label_column] = None\n",
    "    df[explanation_column] = None    \n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        row_text = row[text_column]\n",
    "\n",
    "        unique_id = f\"[Row ID: {idx}]\"\n",
    "        prompt = f\"{unique_id}\\n\\n\" + prompt_template.format(text=row_text)\n",
    "\n",
    "        response = requests.post(\n",
    "            endpoint_url,\n",
    "            headers={'Content-Type': 'application/json'},\n",
    "            json={\n",
    "                'model': model_name,\n",
    "                'prompt': prompt,\n",
    "                'stream': False\n",
    "            })\n",
    "\n",
    "        print(f\"\\n--- Index {idx} ---\")\n",
    "        print(\"Prompt:\")\n",
    "        print(prompt)\n",
    "        print(f\"Status: {response.status_code}\")\n",
    "        print(\"Raw response:\")\n",
    "        print(response.text)\n",
    "\n",
    "        label = None\n",
    "        explanation = None\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            try:\n",
    "                result_json = response.json()\n",
    "                raw_response_str = result_json.get('response', ' ')\n",
    "\n",
    "                cleaned_str = raw_response_str.strip().replace(\"```json\", \" \").replace(\"```\", \" \").strip()\n",
    "                parsed_output = json.loads(cleaned_str)\n",
    "\n",
    "                label = parsed_output.get(label_column)\n",
    "                explanation = parsed_output.get(explanation_column)\n",
    "\n",
    "            except (json.JSONDecodeError, KeyError, TypeError) as e:\n",
    "                print(\"JSON error:\", e)\n",
    "                print(\"Bad string:\")\n",
    "                print(cleaned_str)\n",
    "\n",
    "        # save results by row index for correct matching\n",
    "        \n",
    "        results.append({\n",
    "            'idx': idx,\n",
    "            label_column: label,\n",
    "            explanation_column: explanation\n",
    "            })\n",
    "\n",
    "        # impose delay - avoid model caching errors\n",
    "        \n",
    "        time.sleep(0.25)        \n",
    "        \n",
    "    # create result df - align to input df by row index\n",
    "    \n",
    "    result_df = pd.DataFrame(results).set_index('idx')\n",
    "    df.update(result_df)\n",
    "\n",
    "    return df\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "def calculate_kappa(df, col1, col2):\n",
    "    '''\n",
    "    Computes Cohen's kappa between two columns.\n",
    "    '''\n",
    "    return cohen_kappa_score(df[col1], df[col2])\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "def calculate_percent_agreement(df, col_pairs):\n",
    "    '''\n",
    "    Computes percent agreement for a list of column pairs.\n",
    "    '''\n",
    "    results = {}\n",
    "    for col1, col2 in col_pairs:\n",
    "        agreement = df[col1] == df[col2]\n",
    "        percent_agreement = (agreement.sum() / len(df)) * 100\n",
    "        results[f\"{col1} & {col2}\"] = percent_agreement\n",
    "    return results\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "def encode_disagreements(row):\n",
    "    '''\n",
    "    Returns 1 if two values disagree, 0 otherwise.\n",
    "    '''\n",
    "    return 1 if row[0] != row[1] else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qualitative import( # type: ignore\n",
    "    build_prompt,\n",
    "    code_texts_deductively_ollama,\n",
    "    calculate_kappa,\n",
    "    calculate_percent_agreement,\n",
    "    encode_disagreements,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_acceptability_scatter(df, int_cols, x_labels, y_labels, title, output_path, \n",
    "                                replace_val = None, color = '#7eb0d5', figsize = (8, 5)):\n",
    "    '''\n",
    "    Creates a jittered scatter plot with mean +/- SD overlay for Likert-scale acceptability data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing the intervention columns.\n",
    "    int_cols : list\n",
    "        Column names to plot (e.g., ['int1_rev', 'int2_rev', ...]).\n",
    "    x_labels : list\n",
    "        Labels for x-axis ticks (must match length of int_cols).\n",
    "    y_labels : dict\n",
    "        Mapping of numeric values to y-axis labels (e.g., {1: 'disagree', 2: 'somewhat disagree'}).\n",
    "    title : str\n",
    "        Plot title.\n",
    "    output_path : Path\n",
    "        Full path for saving the figure.\n",
    "    replace_val : int, optional\n",
    "        Value to replace with NaN before plotting (e.g., 5 for \"N/A\" responses).\n",
    "    color : str\n",
    "        Hex color for points and error bars. Default: '#7eb0d5' (ng_blue).\n",
    "    figsize : tuple\n",
    "        Figure dimensions. Default: (8, 5).\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    None (displays and saves figure)\n",
    "    '''\n",
    "    \n",
    "    sns.set_style(style = 'whitegrid', rc = None)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize = figsize)\n",
    "    x_positions = range(len(int_cols))\n",
    "    \n",
    "    for i, col in enumerate(int_cols):\n",
    "        y_vals = df[col].astype(float)\n",
    "        \n",
    "        # handle N/A responses if specified\n",
    "        if replace_val is not None:\n",
    "            y_vals = y_vals.replace(replace_val, np.nan).dropna()\n",
    "        \n",
    "        x_jittered = np.random.normal(\n",
    "            loc = i, \n",
    "            scale = 0.16, \n",
    "            size = len(y_vals),\n",
    "            )\n",
    "        \n",
    "        # scatter points\n",
    "        ax.scatter(\n",
    "            x_jittered, \n",
    "            y_vals, \n",
    "            alpha = 0.6, \n",
    "            s = 35, \n",
    "            color = color, \n",
    "            linewidths = 0.5,\n",
    "            )\n",
    "        \n",
    "        # mean +/- SD overlay\n",
    "        ax.errorbar(\n",
    "            i, \n",
    "            y_vals.mean(), \n",
    "            yerr = y_vals.std(), \n",
    "            fmt = 'D', \n",
    "            color = color,\n",
    "            markersize = 8, \n",
    "            capsize = 0, \n",
    "            linewidth = 1, \n",
    "            zorder = 3,\n",
    "            )\n",
    "    \n",
    "    # format axes\n",
    "    ax.set_xticks(x_positions)\n",
    "    ax.set_xticklabels(\n",
    "        x_labels, \n",
    "        rotation = 45, \n",
    "        ha = 'right', \n",
    "        fontsize = 9,\n",
    "        )\n",
    "    ax.tick_params(\n",
    "        axis = 'y', \n",
    "        left = False, \n",
    "        labelleft = False,\n",
    "        )\n",
    "    \n",
    "    ax_right = ax.secondary_yaxis('right')\n",
    "    ax_right.set_yticks(list(y_labels.keys()))\n",
    "    ax_right.set_yticklabels(list(y_labels.values()))\n",
    "    \n",
    "    ax.set_title(title, fontsize = 10)\n",
    "    ax.grid(False)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # save and display\n",
    "    plt.savefig(output_path, dpi = 300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Describe\n",
    "Aggregates, tabulates, paradata.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2a. Daily diary skills recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_diary = pd.read_csv(DATA_PARADATA / 'diary_response.csv')\n",
    "\n",
    "valid_tx_ids = {\n",
    "    'g1008', 'g1011', 'g1014', 'g1019', 'g1023', 'g1025', 'g1027',\n",
    "    'g1030', 'g1033', 'g1034', 'g1035', 'g1037', 'g1038', 'g1039',\n",
    "    'g1040', 'g1042', 'g1044', 'g1046', 'g1047', 'g1049', 'g1050',\n",
    "    'g1051', 'gg1045'\n",
    "    }\n",
    "\n",
    "# reset idx\n",
    "\n",
    "d = d_diary[d_diary['username'].isin(valid_tx_ids)]\n",
    "d.reset_index(inplace = True, drop = True)\n",
    "\n",
    "# convert datetime\n",
    "\n",
    "d['created_at'] = pd.to_datetime(\n",
    "    d['created_at'], utc = True\n",
    "    ).dt.tz_convert(None).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# inspect\n",
    "\n",
    "d.shape\n",
    "d.info()\n",
    "d.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(d.columns.tolist())\n",
    "#d.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommendation_skill_selection - inspect\n",
    "\n",
    "d['recommendation_skill_selection'] = d['recommendation_skill_selection'].fillna('none')\n",
    "print(d['recommendation_skill_selection'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get n \n",
    "\n",
    "n = d['recommendation_skill_selection'].value_counts(dropna = False)\n",
    "#pct = d['recommendation_skill_selection'].value_counts(\n",
    "#    normalize = True, \n",
    "#    dropna = False,\n",
    "#    ) * 100\n",
    "\n",
    "# merge, display\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    'n': n,\n",
    "#    'pct': pct.round(2) ### round to 2 decimal places\n",
    "    })\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get %: n / N skills recommendations \n",
    "\n",
    "10 / 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2b. Completed skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv(DATA_PARADATA / 'UserSummary.csv')\n",
    "\n",
    "# inspect\n",
    "\n",
    "d.shape\n",
    "d.info()\n",
    "d.head(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * spent_minutes - mdn / iqr\n",
    "\n",
    "skills_min = [\n",
    "    'Breathing Retraining spent_minutes', \n",
    "    'Journal Writing spent_minutes', \n",
    "    'Adaptive Coping spent_minutes',\n",
    "    'Common Irrational Thoughts spent_minutes', \n",
    "    'Choosing a Coping Strategy spent_minutes', \n",
    "    'Set a SMART Goal spent_minutes',\n",
    "    'Meditation spent_minutes',\n",
    "    ]\n",
    "\n",
    "# compute mdn / iqr\n",
    "\n",
    "summary = {}\n",
    "\n",
    "for skill in skills_min:\n",
    "    non_zero = d[d[skill] > 0][skill] ### parse for >0 values\n",
    "    zero_count = (d[skill] == 0).sum() ### count 0 values\n",
    "    mdn = non_zero.median()\n",
    "    q1 = non_zero.quantile(0.25)\n",
    "    q3 = non_zero.quantile(0.75)\n",
    "    val_range = non_zero.max() - non_zero.min() ### range of >0 values\n",
    "    \n",
    "    summary[skill] = {\n",
    "        'mdn': mdn,\n",
    "#        'q1': q1,\n",
    "#        'q3': q3,\n",
    "        'zero_count': zero_count,\n",
    "        'non_zero_range': val_range,       \n",
    "        }\n",
    "\n",
    "# tabulate, display\n",
    "\n",
    "summary = pd.DataFrame(summary).T ### transpose for interpretability\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * completion_count - mdn / iqr\n",
    "\n",
    "skills_complete = [\n",
    "    'Breathing Retraining completion_count', \n",
    "    'Journal Writing completion_count',\n",
    "    'Adaptive Coping completion_count',\n",
    "    'Common Irrational Thoughts completion_count',\n",
    "    'Choosing a Coping Strategy completion_count',\n",
    "    'Set a SMART Goal completion_count',\n",
    "    'Meditation completion_count',\n",
    "    ]\n",
    "\n",
    "# compute m (sd)\n",
    "\n",
    "summary = {}\n",
    "\n",
    "for skill in skills_complete:\n",
    "    non_zero = d[d[skill] > 0][skill] ### parse for >0 values\n",
    "    zero_count = (d[skill] == 0).sum() ### count 0 values\n",
    "#    mean_val = d[col].mean()\n",
    "#    std_val = d[col].std()\n",
    "    mean_val = non_zero.mean()\n",
    "    std_val = non_zero.std()\n",
    "    summary[skill] = {\n",
    "        'mean': mean_val,\n",
    "        'std': std_val,\n",
    "        'zero_count': zero_count,\n",
    "        }\n",
    "\n",
    "# tabulate, display\n",
    "\n",
    "summary = pd.DataFrame(summary).T\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get n\n",
    "\n",
    "sums = {}\n",
    "\n",
    "for skill in skills_complete:\n",
    "    total = d[skill].sum()\n",
    "    sums[skill] = total\n",
    "\n",
    "# tabulate, display\n",
    "\n",
    "d_sums = pd.DataFrame.from_dict(\n",
    "    sums, \n",
    "    orient = 'index', \n",
    "    columns = ['sum'],\n",
    "    )\n",
    "print(d_sums)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get %: n / N skills completed\n",
    "\n",
    "19 / 205"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visualize\n",
    "Creates scatter plots with jittered points and meanÂ±SD overlays.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "d = pd.read_csv(DATA_DIR / 'nola_gem_acceptability_no_loc_tx.csv', index_col = [0])\n",
    "\n",
    "# inspect\n",
    "\n",
    "d.shape\n",
    "d.info()\n",
    "d.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Fig. 1.** NOLA Gem acceptability: overall impressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fig. 1: overall impressions (int1-int5)\n",
    "\n",
    "plot_acceptability_scatter(\n",
    "    df = d,\n",
    "    int_cols = ['int1_rev', 'int2_rev', 'int3_rev', \n",
    "              'int4_rev', 'int5_rev'],\n",
    "    x_labels = ['easy to use', 'useful for coping', 'recommend to a friend', \n",
    "              'satisfied with app', 'use again'],\n",
    "    y_labels = {1: 'disagree', 2: 'somewhat\\ndisagree', 3: 'somewhat\\nagree', 4: 'agree'},\n",
    "    title = 'NOLA Gem acceptability: overall impressions',\n",
    "    output_path = FIGURES_DIR / 'int1_int5_high_res_scatter.png',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqc5gKR0Rfot",
    "tags": []
   },
   "source": [
    "##### **Fig. 2a.** NOLA Gem acceptability: perceived helpfulness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "loTG5glMUaO7"
   },
   "outputs": [],
   "source": [
    "# Fig. 2a: perceived helpfulness (int6-int10)\n",
    "\n",
    "plot_acceptability_scatter(\n",
    "    df = d,\n",
    "    int_cols = ['int6_rev', 'int7_rev', 'int8_rev', \n",
    "              'int9_rev', 'int10_rev'],\n",
    "    x_labels = ['reducing distress', 'improving mood', 'facilitating coping', \n",
    "              'changing habits', 'learning new skills'],\n",
    "    y_labels = {1: 'disagree', 2: 'somewhat\\ndisagree', 3: 'somewhat\\nagree', 4: 'agree'},\n",
    "    title = '($a.$) I found NOLA Gem to be helpful in...',\n",
    "    output_path = FIGURES_DIR / 'int6_int10_high_res_scatter.png',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Fig. 2b.** NOLA Gem acceptability: perceived helpfulness of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_Y_hMMjFZy2"
   },
   "outputs": [],
   "source": [
    "# Fig. 2b: perceived helpfulness of features (int11-int14)\n",
    "\n",
    "plot_acceptability_scatter(\n",
    "    df = d,\n",
    "    int_cols = ['int11_rev', 'int12_rev', 'int13_rev', \n",
    "              'int14_rev'],\n",
    "    x_labels = ['educational sessions', 'skills practice', 'geofencing alerts', \n",
    "              'daily diary suggested skills'],\n",
    "    y_labels = {1: 'not very helpful', 2: 'a little\\nhelpful', 3: 'somewhat\\nhelpful', 4: 'very helpful'},\n",
    "    title = '($b.$) I found the NOLA Gem __________ features to be...$^a$',\n",
    "    output_path = FIGURES_DIR / 'int11_int14_high_res_scatter.png',\n",
    "    replace_val = 5,  # handle N/A responses\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Code\n",
    "Local LLM x human synergistic deductive qualitative coding with on-device Ollama model (Gemma 12B).\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4a. Transform qual-ready df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create qual df from main data\n",
    "\n",
    "d_qual = d[['open1', 'open2', 'open3', 'open4']].copy()\n",
    "\n",
    "# slice by open-ended item\n",
    "\n",
    "d_qual_open1 = d_qual[['open1']].copy()\n",
    "d_qual_open2 = d_qual[['open2']].copy()\n",
    "d_qual_open3 = d_qual[['open3']].copy()\n",
    "d_qual_open4 = d_qual[['open4']].copy()\n",
    "\n",
    "# inspect\n",
    "\n",
    "d_qual.info()\n",
    "d_qual.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert qual code / rationale cols\n",
    "\n",
    "d_qual[[\n",
    "    'fctn_sjs', 'fctn_rtnl_sjs', 'lgth_sjs', 'lgth_rtnl_sjs',\n",
    "    'tmng_sjs', 'tmng_rtnl_sjs', 'attn_sjs', 'attn_rtnl_sjs',\n",
    "    'gltc_sjs', 'gltc_rtnl_sjs', 'prfc_sjs', 'prfc_rtnl_sjs',\n",
    "    'ftrs_sjs', 'ftrs_rtnl_sjs', 'strs_sjs', 'strs_rtnl_sjs',\n",
    "    ]] = ' '\n",
    "\n",
    "d_qual.info()\n",
    "d_qual.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify 'item' col for long df\n",
    "\n",
    "d_qual_open1['item'] = 'open1'\n",
    "d_qual_open2['item'] = 'open2'\n",
    "d_qual_open3['item'] = 'open3'\n",
    "d_qual_open4['item'] = 'open4'\n",
    "\n",
    "# rename all qual cols: 'text'\n",
    "\n",
    "d_qual_open1 = d_qual_open1.rename(columns = {'open1': 'text'})\n",
    "d_qual_open2 = d_qual_open2.rename(columns = {'open2': 'text'})\n",
    "d_qual_open3 = d_qual_open3.rename(columns = {'open3': 'text'})\n",
    "d_qual_open4 = d_qual_open4.rename(columns = {'open4': 'text'})\n",
    "\n",
    "# concatenate: long df\n",
    "\n",
    "d_qual_long = pd.concat(\n",
    "    [d_qual_open1, d_qual_open2,\n",
    "     d_qual_open3, d_qual_open4],\n",
    "    axis = 0,\n",
    "    ignore_index = False,\n",
    "    )\n",
    "\n",
    "# reorder\n",
    "\n",
    "d_qual_long = d_qual_long.reindex(columns = ['item', 'text',\n",
    "    'fctn_sjs', 'fctn_rtnl_sjs', 'lgth_sjs', 'lgth_rtnl_sjs', 'tmng_sjs', 'tmng_rtnl_sjs',\n",
    "    'attn_sjs', 'attn_rtnl_sjs', 'gltc_sjs', 'gltc_rtnl_sjs', 'prfc_sjs', 'prfc_rtnl_sjs',\n",
    "    'ftrs_sjs', 'ftrs_rtnl_sjs', 'strs_sjs', 'strs_rtnl_sjs',\n",
    "    ])\n",
    "\n",
    "d_qual_long.info()\n",
    "d_qual_long.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "d_qual_long.to_excel(DATA_DIR / 'nola_gem_acceptability_qual.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4b. Formulate deductive coding prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# role / text template - shared across all prompts\n",
    "\n",
    "role = '''\n",
    "    You are tasked with applying pre-defined qualitative codes to segments of text excerpted \n",
    "    from interviews with users of a mental health app for people living with HIV. \n",
    "    \n",
    "    The app included text-messaged surveys multiple times a day, as well as self-directed sessions \n",
    "    on mindfulness meditation, trauma psychoeducation (etc.), and momentary prompts to engage with \n",
    "    brief coping skills training as needed throughout the day. \n",
    "    \n",
    "    You will be provided a definition, instructions, and \n",
    "    key exemplars of text to guide your coding decisions.\n",
    "    '''\n",
    "\n",
    "text = '''\n",
    "    ---\n",
    "    Text to classify:\n",
    "    \"{text}\"\n",
    "    ---\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _EMA questionnaire length_ (alias: `lgth`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgth_prompt = build_prompt(\n",
    "    code = 'EMA questionnaire length',\n",
    "    alias = 'lgth',\n",
    "    code_def = '''\n",
    "        Describes frustration with text message survey, daily diary, or questionnaire (all refer to the same \n",
    "        measure) length. These frustrations can include aspects such as the number of questions, or the amount \n",
    "        of time allotted to complete the questionnaire, or the nature of the questions administered to participants.\n",
    "    \n",
    "        Frustrations must be directly attributable to the duration of the survey or the time it takes to respond \n",
    "        to, specifically and explicitly. Vague dissatisfaction with the survey does not warrant a 'lgth' = 1. \n",
    "        ''',\n",
    "    code_ex = '''\n",
    "        - \"The questions they texted you, there was just too manyâ€¦the questions seemed to ask the same thing \n",
    "        over and again\"\n",
    "        - \"The number of questions was too many. It's inconvenient especially if you have typical work and \n",
    "        family responsibilities\"\n",
    "        - \"There are so many questions on the survey and they don't give you enough time to finish them all\"\n",
    "        ''',\n",
    "    )\n",
    "\n",
    "print(lgth_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbOPu9Mi21Kk"
   },
   "source": [
    "##### _EMA prompt timing_ (alias: `tmng`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmng_prompt = build_prompt(\n",
    "    code = 'EMA prompt timing',\n",
    "    alias = 'tmng',\n",
    "    code_def = '''\n",
    "        Describes frustration with text message survey, daily diary, or questionnaire (all refer to the same \n",
    "        measure) timing during the day. These frustrations can include the questionnaires being sent too early, \n",
    "        or too late, or being difficult to fit in due to obligations such as a full-time job.\n",
    "    \n",
    "        Frustrations must be directly attributable to the timing of the survey delivery, specifically\n",
    "        and explicitly. Vague dissatisfaction with the survey does not warrant a 'tmng' = 1. \n",
    "        ''',\n",
    "    code_ex = '''\n",
    "         - \"I couldnâ€™t always answer the surveys when they sent them during the dayâ€¦because Iâ€™ve got a \n",
    "         full-time job\"\n",
    "        - \"The timing was all wrong. The diaries should come in earlier or later in the day so I can \n",
    "        answer without disrupting my schedule\"\n",
    "        - \"The time periods for the text message surveys was unworkable for me. Timing would be better if \n",
    "        you could choose yourself\"\n",
    "        ''',\n",
    "    )\n",
    "\n",
    "print(tmng_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _EMA item attunement_ (alias: `attn`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_prompt = build_prompt(\n",
    "    code = 'EMA item attunement',\n",
    "    alias = 'attn',\n",
    "    code_def = '''\n",
    "        Recounts opinions that text message survey, daily diary, or questionnaire (all refer to the same \n",
    "        measure) were inappropriate to or incongruent with app usersâ€™ lived experience, including recommendations \n",
    "        for more appropriate questions or question wording.\n",
    "    \n",
    "        The text must refer to the mismatch between item wording and the user's life experience, explicitly and \n",
    "        specifically, to warrant a 'attn' = 1. Mentions of formatting issues such as the survey length or timing, \n",
    "        alone, do _not_ warrant a 'attn' = 1\n",
    "        ''',\n",
    "    code_ex = '''\n",
    "        - \"Not all of the questions made sense to me, or really fit into how I think about my life\"\n",
    "        - \"The questions felt too vague. I wasn't sure what they meant\"\n",
    "        - \"More questions on prayer and surivival, less about drugs and violence\"\n",
    "        ''',\n",
    "    )\n",
    "\n",
    "print(attn_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _\"Gotta get those glitches fixed_ [in vivo]_\"_ (alias: `gltc`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gltc_prompt = build_prompt(\n",
    "    code = 'Glitch fixes',\n",
    "    alias = 'gltc',\n",
    "    code_def = '''\n",
    "        Describes any app (including text message delivery) malfunctions.\n",
    "        ''',\n",
    "    code_ex = '''\n",
    "        - \"It just froze while I was answering my survey one day\"\n",
    "        - \"I feel like it could be great but it felt too glitchy for now\"\n",
    "        - \"The glitches. I just couldn't deal with the glitches\"\n",
    "        ''',\n",
    "    )\n",
    "\n",
    "print(gltc_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6pDbOL73zTT"
   },
   "source": [
    "##### \"_This app is perfect_ [in vivo]\" (alias: `prfc`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prfc_prompt = build_prompt(\n",
    "    code = 'Perfection',\n",
    "    alias = 'prfc',\n",
    "    code_def = '''\n",
    "        Captures all responses that express satisfaction with the pilot app or reject any suggestions for \n",
    "        updated features.\n",
    "    \n",
    "        Positive sentiment towards the app, alone, does _not_ warrant a 'prfc' = 1; statements that nothing \n",
    "        needs to be changed must be specific and explicit to warrant a 'prfc' = 1.\n",
    "        ''',\n",
    "    code_ex = '''\n",
    "        - \"I don't think you need to put in any changes at all\"\n",
    "        - \"There was nothing I didn't like\"\n",
    "        - \"I liked it all\"\n",
    "        ''',\n",
    "    )\n",
    "\n",
    "print(prfc_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _Tweaks and fresh features_ (alias: `ftrs`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftrs_prompt = build_prompt(\n",
    "    code = 'Tweaks and fresh features',\n",
    "    alias = 'ftrs',\n",
    "    code_def = '''\n",
    "        Captures all specific recommendations for alterations to existing app features or expressed wishes for \n",
    "        entirely new features to be added to the app.\n",
    "    \n",
    "        New features must be suggested clearly and specifically to warrant a 'ftrs' = 1; dissatisfaction with \n",
    "        existing features alone does _not_ warrant a 'ftrs' = 1.\n",
    "        ''',\n",
    "    code_ex = '''\n",
    "        - \"More coping skill should be added\"\n",
    "        - \"I wish it had less text, more multimedia\"\n",
    "        - \"The questions should be more personalized use to user to better fit our needs\"\n",
    "        ''',\n",
    "    )\n",
    "\n",
    "print(ftrs_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _â€œYou seem a little stressedâ€¦it will tell you what you can do_ [in vivo]_â€_ (alias: `strs`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strs_prompt = build_prompt(\n",
    "    code = 'JITAI recognition',\n",
    "    alias = 'strs',\n",
    "    code_def = '''\n",
    "        Captures any expression of appreciation for the app's features being offered when needed \n",
    "        throughout the day, or tailored to particular stressors reported by the user.\n",
    "        \n",
    "        Positive sentiment towards the app, alone, does _not_ warrant a 'strs' = 1; descriptions \n",
    "        of the app being uniquely responsive to users' needs must be specific and explicit.\n",
    "        ''',\n",
    "    code_ex = '''\n",
    "        - \"I liked how different meditations were offered for different stresses you mention throughout the day\"\n",
    "        - \"The daily check-ins and quick follow ups to try and help you right when you most need it\"\n",
    "        - \"Learning all these new skills to stay calm and collected during the day\"\n",
    "        ''',\n",
    "    )\n",
    "\n",
    "print(strs_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4c. Dual code human-coded qual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_excel(DATA_QUAL / 'nola_gem_acceptability_qual_sjs.xlsx', index_col = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "\n",
    "# locally hosted Ollama endpoint\n",
    "\n",
    "ollama_endpoint = 'http://localhost:11434/api/generate'\n",
    "\n",
    "# define aliases, prompts\n",
    "\n",
    "prompts = [\n",
    "    ('lgth', lgth_prompt), ### (alias, prompt_template)\n",
    "    ('tmng', tmng_prompt),\n",
    "    ('attn', attn_prompt),\n",
    "    ('gltc', gltc_prompt),\n",
    "    ('prfc', prfc_prompt),\n",
    "    ('ftrs', ftrs_prompt),\n",
    "    ('strs', strs_prompt),\n",
    "    ]\n",
    "\n",
    "# loop through each alias, prompt\n",
    "\n",
    "for alias, prompt_template in prompts:\n",
    "\n",
    "# apply code_texts_deductively_ollama over aliases, prompts, updated_df\n",
    "\n",
    "    d_coded = code_texts_deductively_ollama(\n",
    "        d,\n",
    "        alias = alias,\n",
    "        text_column = 'text',\n",
    "        endpoint_url = ollama_endpoint,\n",
    "        prompt_template = prompt_template,\n",
    "        model_name = 'gemma3:12B',\n",
    "        )\n",
    "\n",
    "d_coded.head(3)\n",
    "\n",
    "# export\n",
    "\n",
    "d_coded.to_excel('d_coded.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4d. Compute Cohen's $\\kappa$ / % agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_excel(DATA_QUAL / 'd_coded.xlsx', index_col = [0])\n",
    "\n",
    "encodings_sjs = [\n",
    "    'lgth_sjs', 'tmng_sjs','attn_sjs','gltc_sjs',\n",
    "    'prfc_sjs', 'ftrs_sjs', 'strs_sjs',\n",
    "    ]\n",
    "\n",
    "# numeric conversion - coerce\n",
    "\n",
    "for e in encodings_sjs:\n",
    "    d[e] = pd.to_numeric(d[e], errors = 'coerce')\n",
    "\n",
    "# replace NaN w/ 0    \n",
    "    \n",
    "d[encodings_sjs] = d[encodings_sjs].fillna(0)\n",
    "\n",
    "# inspect\n",
    "\n",
    "d.info()\n",
    "d.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define kappa fx\n",
    "\n",
    "def calculate_kappa(d, col1, col2):\n",
    "    return cohen_kappa_score(d[col1], d[col2])\n",
    "\n",
    "col_pairs = [\n",
    "    ('lgth_sjs', 'lgth_llm'), \n",
    "    ('tmng_sjs', 'tmng_llm'),\n",
    "    ('attn_sjs', 'attn_llm'),\n",
    "    ('gltc_sjs', 'gltc_llm'),\n",
    "    ('prfc_sjs', 'prfc_llm'),\n",
    "    ('ftrs_sjs', 'ftrs_llm'),\n",
    "    ('strs_sjs', 'strs_llm'),\n",
    "    ]\n",
    "\n",
    "# initialize dict\n",
    "\n",
    "kappa_results = {}\n",
    "\n",
    "# % agreement loop\n",
    "\n",
    "def calculate_percent_agreement(df, col_pairs):\n",
    "    results = {}\n",
    "    for col1, col2 in col_pairs:\n",
    "        agreement = df[col1] == df[col2]\n",
    "        percent_agreement = (agreement.sum() / len(df)) * 100\n",
    "        results[f\"{col1} & {col2}\"] = percent_agreement\n",
    "    return results\n",
    "\n",
    "percent_agreement_results = calculate_percent_agreement(d, col_pairs)\n",
    "\n",
    "for pair, percent in percent_agreement_results.items():\n",
    "    print(f\"Percent agreement for {pair}: {percent:.2f}%\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# kappa loop\n",
    "\n",
    "for col1, col2 in col_pairs:\n",
    "    kappa = calculate_kappa(d, col1, col2)\n",
    "    kappa_results[f'{col1} and {col2}'] = kappa\n",
    "\n",
    "for pair, kappa in kappa_results.items():\n",
    "    print(f\"Cohen's Kappa for {pair}: {kappa:.2f}\")\n",
    "    \n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute Cohen's kappa for each code\n",
    "\n",
    "codes = [\n",
    "    'lgth', 'tmng', 'attn', 'gltc', \n",
    "    'prfc', 'ftrs', 'strs',\n",
    "    ]\n",
    "\n",
    "kappa_results = {}\n",
    "\n",
    "for code in codes:\n",
    "    sjs_col = f'{code}_sjs'\n",
    "    llm_col = f'{code}_llm'\n",
    "    \n",
    "    if sjs_col in d.columns and llm_col in d.columns:\n",
    "        # drop rows with NaN in either column\n",
    "        valid_mask = d[sjs_col].notna() & d[llm_col].notna()\n",
    "        sjs_ratings = d.loc[valid_mask, sjs_col].astype(int)\n",
    "        llm_ratings = d.loc[valid_mask, llm_col].astype(int)\n",
    "        \n",
    "        if len(sjs_ratings) > 0:\n",
    "            kappa = cohen_kappa_score(sjs_ratings, llm_ratings)\n",
    "            kappa_results[code] = kappa\n",
    "            print(f\"{code}: kappa = {kappa:.3f} (n = {len(sjs_ratings)})\")\n",
    "\n",
    "# summary\n",
    "\n",
    "d_kappa = pd.DataFrame.from_dict(\n",
    "    kappa_results, \n",
    "    orient = 'index', \n",
    "    columns = ['kappa'],\n",
    "    )\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(d_kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag disagreements\n",
    "\n",
    "#print(d.columns.tolist())\n",
    "\n",
    "def encode_disagreements(row):\n",
    "    return 1 if row[0] != row[1] else 0\n",
    "\n",
    "col_dis = [\n",
    "    ('lgth_sjs', 'lgth_llm', 'lgth_dis'), \n",
    "    ('tmng_sjs', 'tmng_llm', 'tmng_dis'),\n",
    "    ('attn_sjs', 'attn_llm', 'attn_dis'),\n",
    "    ('gltc_sjs', 'gltc_llm', 'gltc_dis'),\n",
    "    ('prfc_sjs', 'prfc_llm', 'prfc_dis'),\n",
    "    ('strs_sjs', 'strs_llm', 'strs_dis'),\n",
    "    ]\n",
    "\n",
    "for col1, col2, dis_col in col_dis:\n",
    "    d[dis_col] = d[[col1, col2]].apply(\n",
    "        encode_disagreements,\n",
    "        axis = 1,\n",
    "        )\n",
    "\n",
    "# obfuscate human vs. Gemma encodings    \n",
    "    \n",
    "# Example: rename _sjs to _a and _llm to _b\n",
    "\n",
    "#d.columns = [\n",
    "#    col.replace('_sjs', '_a').replace('_llm', '_b') if col.endswith(('_sjs', '_llm')) else col\n",
    "#    for col in d.columns\n",
    "#    ]\n",
    "\n",
    "# reorder for ST interpretability \n",
    "\n",
    "d = d[[\n",
    "    'item', 'text', \n",
    "    'lgth_sjs', 'lgth_rtnl_sjs', 'lgth_llm', 'lgth_expl','lgth_dis', \n",
    "    'tmng_sjs', 'tmng_rtnl_sjs', 'tmng_llm', 'tmng_expl', 'tmng_dis', \n",
    "    'attn_sjs', 'attn_rtnl_sjs', 'attn_llm', 'attn_expl', 'attn_dis', \n",
    "    'gltc_sjs', 'gltc_rtnl_sjs','gltc_llm', 'gltc_expl', 'gltc_dis', \n",
    "    'prfc_sjs', 'prfc_rtnl_sjs','prfc_llm', 'prfc_expl', 'prfc_dis', \n",
    "    'strs_sjs', 'strs_rtnl_sjs', 'strs_llm', 'strs_expl', 'strs_dis',\n",
    "    ]]\n",
    "\n",
    "# inspect\n",
    "\n",
    "d.info()\n",
    "d.head(3)    \n",
    "        \n",
    "# export\n",
    "\n",
    "d.to_excel(f'd_coded_icr.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> End of `nola_gem_acceptability.ipynb`"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "RS6CgqU2RO4r",
    "qqc5gKR0Rfot",
    "drYSb9WIT-fv"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
